{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"openai_api_key\"] = \"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\"\n",
    "\n",
    "loader = CSVLoader(file_path='test_dataset.csv')\n",
    "\n",
    "index_creator = VectorstoreIndexCreator()\n",
    "docsearch = index_creator.from_loaders([loader])\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-980AIMcMfFUQfpqBSW68ayfU on requests per day. Limit: 200 / day. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-980AIMcMfFUQfpqBSW68ayfU on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-980AIMcMfFUQfpqBSW68ayfU on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shahs\\HOME\\GITHUB\\DATAHACK 2.0\\gpt_killer_model.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shahs/HOME/GITHUB/DATAHACK%202.0/gpt_killer_model.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m query\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnter your query: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shahs/HOME/GITHUB/DATAHACK%202.0/gpt_killer_model.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtell me  to opt for and some detials of the lawyer and why he/she will be best for the case, give 1 male and 1 female lawyer for case in equal gender proportion\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shahs/HOME/GITHUB/DATAHACK%202.0/gpt_killer_model.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response \u001b[39m=\u001b[39m  chain({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m:query\u001b[39m+\u001b[39;49mtext})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shahs/HOME/GITHUB/DATAHACK%202.0/gpt_killer_model.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    299\u001b[0m     inputs,\n\u001b[0;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:136\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    132\u001b[0m accepts_run_manager \u001b[39m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs)\u001b[39m.\u001b[39mparameters\n\u001b[0;32m    134\u001b[0m )\n\u001b[0;32m    135\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager:\n\u001b[1;32m--> 136\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_docs(question, run_manager\u001b[39m=\u001b[39;49m_run_manager)\n\u001b[0;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs(question)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:216\u001b[0m, in \u001b[0;36mRetrievalQA._get_docs\u001b[1;34m(self, question, run_manager)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_docs\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     question: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    212\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m    213\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[0;32m    214\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    215\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretriever\u001b[39m.\u001b[39;49mget_relevant_documents(\n\u001b[0;32m    217\u001b[0m         question, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[0;32m    218\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\schema\\retriever.py:204\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m _kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_other_args \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 204\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_relevant_documents(\n\u001b[0;32m    205\u001b[0m         query, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs\n\u001b[0;32m    206\u001b[0m     )\n\u001b[0;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_relevant_documents(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\schema\\vectorstore.py:585\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_relevant_documents\u001b[39m(\n\u001b[0;32m    582\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m    583\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    584\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 585\u001b[0m         docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49msimilarity_search(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_kwargs)\n\u001b[0;32m    586\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity_score_threshold\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    587\u001b[0m         docs_and_similarities \u001b[39m=\u001b[39m (\n\u001b[0;32m    588\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m    589\u001b[0m                 query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_kwargs\n\u001b[0;32m    590\u001b[0m             )\n\u001b[0;32m    591\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\vectorstores\\chroma.py:261\u001b[0m, in \u001b[0;36mChroma.similarity_search\u001b[1;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search\u001b[39m(\n\u001b[0;32m    245\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    246\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    250\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    251\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[0;32m    253\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39m        List[Document]: List of documents most similar to the query text.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m     docs_and_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimilarity_search_with_score(query, k, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m)\n\u001b[0;32m    262\u001b[0m     \u001b[39mreturn\u001b[39;00m [doc \u001b[39mfor\u001b[39;00m doc, _ \u001b[39min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\vectorstores\\chroma.py:345\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[1;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__query_collection(\n\u001b[0;32m    339\u001b[0m         query_texts\u001b[39m=\u001b[39m[query],\n\u001b[0;32m    340\u001b[0m         n_results\u001b[39m=\u001b[39mk,\n\u001b[0;32m    341\u001b[0m         where\u001b[39m=\u001b[39m\u001b[39mfilter\u001b[39m,\n\u001b[0;32m    342\u001b[0m         where_document\u001b[39m=\u001b[39mwhere_document,\n\u001b[0;32m    343\u001b[0m     )\n\u001b[0;32m    344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m     query_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_query(query)\n\u001b[0;32m    346\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__query_collection(\n\u001b[0;32m    347\u001b[0m         query_embeddings\u001b[39m=\u001b[39m[query_embedding],\n\u001b[0;32m    348\u001b[0m         n_results\u001b[39m=\u001b[39mk,\n\u001b[0;32m    349\u001b[0m         where\u001b[39m=\u001b[39m\u001b[39mfilter\u001b[39m,\n\u001b[0;32m    350\u001b[0m         where_document\u001b[39m=\u001b[39mwhere_document,\n\u001b[0;32m    351\u001b[0m     )\n\u001b[0;32m    353\u001b[0m \u001b[39mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\embeddings\\openai.py:518\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_query\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[0;32m    510\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \n\u001b[0;32m    512\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[39m        Embedding for the text.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_documents([text])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\embeddings\\openai.py:490\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\embeddings\\openai.py:374\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    371\u001b[0m     _iter \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(tokens), _chunk_size)\n\u001b[0;32m    373\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m _iter:\n\u001b[1;32m--> 374\u001b[0m     response \u001b[39m=\u001b[39m embed_with_retry(\n\u001b[0;32m    375\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    376\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokens[i : i \u001b[39m+\u001b[39;49m _chunk_size],\n\u001b[0;32m    377\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invocation_params,\n\u001b[0;32m    378\u001b[0m     )\n\u001b[0;32m    379\u001b[0m     batched_embeddings\u001b[39m.\u001b[39mextend(r[\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    381\u001b[0m results: List[List[List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(texts))]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\embeddings\\openai.py:107\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     response \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m _check_response(response, skip_empty\u001b[39m=\u001b[39membeddings\u001b[39m.\u001b[39mskip_empty)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m _embed_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tenacity\\__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    388\u001b[0m     retry_state\u001b[39m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msleep(do)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m do\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msleep\u001b[39m(seconds: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[39m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     time\u001b[39m.\u001b[39msleep(seconds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query=input(\"Enter your query: \")\n",
    "text=\"tell me  to opt for and some detials of the lawyer and why he/she will be best for the case, give 1 male and 1 female lawyer for case in equal gender proportion\"\n",
    "response =  chain({\"question\":query+text})\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Two brothers were co-owners of a commercial property. One brother had a son and a daughter (both Nabalig), while his wife was alive. After his wife died, his children were with their mother. A few years later, the husband (the co-owner) died. Can the surviving brother (the co-owner) claim rights to the children?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "\n",
    "language='hindi'\n",
    "command='दो भाइयों को एक कमर्शियल प्रॉपर्टी के मालिक का किरायेदार था। एक भाई के पास एक बेटा और एक बेटी थी (दोनों नाबालिग) जब उसकी पत्नी से तलाक हो गया। तलाक के समय बच्चे माँ के होते हैं। कुछ सालों बाद, पति (सह-किरायेदार) की मृत्यु हो गई। क्या शावक भाई (सह-किरायेदार) के बच्चों को अधिकार का दावा कर सकता है?'\n",
    "text='convert this to english'\n",
    "message = command+text\n",
    "if message:\n",
    "    try:\n",
    "        chat = openai.Completion.create(\n",
    "            model=\"text-davinci-002\",\n",
    "            prompt=\"Assistant: \" + message,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        reply = chat.choices[0].text\n",
    "        response_message = f\"{reply}\"\n",
    "        print(response_message)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "text=\" tell me to opt for and some detials of the lawyer and why he/she will be best for the case, give the list of 10 best lawyers for case in equal gender proportion\"\n",
    "response =  chain({\"question\":response_message+text})\n",
    "response=response['result']\n",
    "\n",
    "text='convert this to hindi'\n",
    "message = response+text\n",
    "if message:\n",
    "    try:\n",
    "        chat = openai.Completion.create(\n",
    "            model=\"text-davinci-002\",\n",
    "            prompt=\"Assistant: \" + message,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        reply = chat.choices[0].text\n",
    "        response_message = f\"{reply}\"\n",
    "        print(response_message)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm sorry, I don't know the answer to this question. You should consult a lawyer for legal advice on this matter.\n",
      "hello\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=input(\"Enter your query: \")\n",
    "text=\"tell me  to opt for and some detials of the lawyer and why he/she will be best for the case, give the list of 10 best lawyers for case in equal gender proportion\"\n",
    "response =  chain({\"question\":query+text})\n",
    "\n",
    "print(response['result'])\n",
    "\n",
    "import openai\n",
    "import json\n",
    "\n",
    "text=\" just convert this data into a list of ditionaries containing the name and why that lawyer was chosen in the format like this: [{'name':'xyz','reason':'abc'},{'name':'xyz','reason':'abc'}]\"\n",
    "message = response['result']+text\n",
    "\n",
    "\n",
    "try:\n",
    "    chat = openai.Completion.create(\n",
    "        model=\"text-davinci-002\",\n",
    "        prompt=\"Assistant: \" + message,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    reply = chat.choices[0].text\n",
    "    response_message = f\"{reply}\"\n",
    "    print(\"hello\")\n",
    "    print(response_message)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
